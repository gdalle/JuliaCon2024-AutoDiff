---
title: "Gradients for everyone"
subtitle: "A quick guide to autodiff in Julia"
author:
  - name: Guillaume Dalle
    orcid: 0000-0003-4866-1687
    email: guillaume.dalle@epfl.ch
    affiliation: 
      - name: EPFL
      - department: IdePHICS, INDY & SPOC laboratories
  - name: Adrian Hill
    orcid: 0009-0009-5977-301X
    email: hill@tu-berlin.de
    affiliation: 
      - name: TU Berlin
      - department: Machine Learning group
date: "2024-07-11"
bibliography: AutoDiff.bib
engine: julia
format:
  revealjs:
    theme: [default, custom.scss]
    toc: true
    toc-depth: 1
    slide-number: true
    overview: true
    code-line-numbers: false
execute:
    echo: true
---


## Follow along
<!-- Ideally, we figure out how to put this on the title slide -->
Slides available on GitHub: 

![](img/qr/qrcode.png)

[github.com/gdalle/JuliaCon2024-AutoDiff](https://github.com/gdalle/JuliaCon2024-AutoDiff)

# Introduction

## Motivation

- What is a derivative?
- Why do we care?

## Differentiable programming

- A function is not just math
- Blocks of code are functions too

## Three types of AD users

1. **Package users** want to differentiate through functions
2. **Package developers** want to write differentiable functions
3. **Backend developers** want to create new AD systems

## Python vs. Julia

- In Python, things are simple and boring: JAX, PyTorch or TensorFlow
- In Julia, things are complicated and fun: <https://juliadiff.org/>
- A lot of folklore and hidden knowledge, hard to see straight
- How to choose an AD backend and make it work?

# How AD works

## A multifaceted concept

See @blondelElementsDifferentiableProgramming2024 for a deep dive:

## Numeric differentiation

## Symbolic differentiation

## Algorithmic differentiation
* Let's look at a function $h(x)=g(f(x))$ composed from two differentiable functions $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and $g: \mathbb{R}^m \rightarrow \mathbb{R}^p$.
  $$
  h = g \circ f\
  $$

* The Jacobian of $h$ can be obtained from the Jacobians of $g$ and $f$ using the **chain rule**:

  $$
  J_h\big|_x = J_{g}\big|_{f(x)} \cdot J_{f}\big|_{x}
  $$

* For functions with "deeper" compositional structure, we need to compute the products of many Jacobians.
  * If implemented naively, full Jacobians requires a lot of memory
  * **Instead, let's do this matrix-free using JVPs and VJPs!**


## Forward mode -- Theory
Forward accumulation on a function $f(x) = f^N(\ldots f^2(f^1(x)))$:

```{julia}
#| echo: false
using Kroki # maybe svgbob can be used directly inside revealjs?
svgbob"""
 "x"  ┌─────┐ "h₁" ┌─────┐"h₂"    "hₙ₋₁"┌─────┐  "y"
──┬──►│ "f¹"├──┬──►│ "f²"├───► .. ──┬──►│ "fⁿ"├───────►
  │   └─────┘  │   └─────┘          │   └─────┘
  │   ┌─────┐  │   ┌─────┐          │   ┌─────┐
  └──►│"𝒟f¹"│  └──►│"𝒟f²"│          └──►│"𝒟fⁿ"│
─────►│     ├─────►│     ├───► .. ─────►│     ├───────►
 "v"  └─────┘      └─────┘              └─────┘"𝒟fₓ(v)"
"""
```

Common names for functions `𝒟f`:

* Jacobian-vector product (JVP)
* pushforward
* forward-rule (`frule`)

## Forward mode -- Implementation


## Reverse mode -- Theory
Reverse accumulation on a function $f(x) = f^N(\ldots f^2(f^1(x)))$:
```{julia}
#| echo: false
using Kroki # maybe svgbob can be used directly inside revealjs?
svgbob"""
    "x"   ┌──────┐ "h₁" ┌──────┐"h₂"    "hₙ₋₁"┌──────┐ "y"
──────┬──►│  "f¹"├──┬──►│  "f²"├───► .. ──┬──►│  "fⁿ"├───►
      │   └──────┘  │   └──────┘          │   └──────┘
      │   ┌──────┐  │   ┌──────┐          │   ┌──────┐
      └──►│"𝒟ᵀf¹"│  └──►│"𝒟ᵀf²"│          └──►│"𝒟ᵀfⁿ"│
◄─────────┤      │◄─────┤      │◄─── .. ◄─────┤      │◄───
"𝒟ᵀfₓ(w)" └──────┘      └──────┘              └──────┘ "w"
"""
```

Common names for functions `𝒟ᵀf`:

* Vector-Jacobian product (VJP)
* pullback
* reverse-rule (`rrule`)


## Reverse mode -- Implementation

# Using AD

## Why so many backends?

- Conflicting paradigms:
  - numeric vs. symbolic vs. algorithmic
  - source-to-source vs. operator overloading
  - Julia level vs. LLVM level
- Cover varying subsets of the language
- Historical reasons: developed by different people

## Useful criteria

- Does this backend execute without error?
- Does it return the right derivative?
- Does it run fast enough for me?

## A simple decision tree

1. Follow recommendations of high-level library (e.g. Flux.jl)
2. Otherwise, choose between forward and reverse mode based on input and output dimensions
3. Try out the most battle-tested backends:

:::: {.columns}

::: {.column width="50%"}
Forward mode

- ForwardDiff
- Enzyme
:::

::: {.column width="50%"}
Reverse mode

- Zygote
- Enzyme
:::

4. If nothing works, fall back on finite differences

::::

## DifferentiationInterface

# Enabling AD

## Typical failures

- `Float64` conversion in ForwardDiff
- Mutation in Zygote
- Ugly LLVM vomit in Enzyme

## ForwardDiff: type-genericity

## Zygote: custom rules

## Enzyme: type-stability

## DifferentiationInterfaceTest

# Developing AD

## What's the point?

- Why don't we all use Enzyme?
- Because 3 years ago it was "why don't we all use Diffractor / Zygote?"
- New solutions like Tapir or FastDifferentiation enable different tradeoffs

## DifferentiationInterface & DifferentiationInterfaceTest

- Sometimes, a little performance loss is acceptable for a simpler API
- Systematic test suites catch many errors

# Appendix

## References

::: {#refs}
:::