---
title: "Gradients for everyone"
subtitle: "A quick guide to autodiff in Julia"
author:
  - name: Guillaume Dalle
    orcid: 0000-0003-4866-1687
    email: guillaume.dalle@epfl.ch
    affiliation: 
      - name: EPFL
      - department: IdePHICS, INDY & SPOC laboratories
  - name: Adrian Hill
    orcid: 0009-0009-5977-301X
    email: hill@tu-berlin.de
    affiliation: 
      - name: TU Berlin
      - department: Machine Learning group
date: "2024-07-11"
bibliography: AutoDiff.bib
engine: julia
format:
  revealjs:
    theme: [default, custom.scss]
    toc: true
    toc-depth: 1
    slide-number: true
    overview: true
    code-line-numbers: false
execute:
    echo: true
---


## Follow along
<!-- Ideally, we figure out how to put this on the title slide -->
Slides available on GitHub: 

![](img/qr/qrcode.png)

[github.com/gdalle/JuliaCon2024-AutoDiff](https://github.com/gdalle/JuliaCon2024-AutoDiff)

# Introduction

## Motivation

- What is a derivative?
- Why do we care?

## Differentiable programming

- A function is not just math
- Blocks of code are functions too

## Three types of AD users

1. **Package users** want to differentiate through functions
2. **Package developers** want to write differentiable functions
3. **Backend developers** want to create new AD systems

## Python vs. Julia

- In Python, things are simple and boring: JAX, PyTorch or TensorFlow
- In Julia, things are complicated and fun: <https://juliadiff.org/>
- A lot of folklore and hidden knowledge, hard to see straight
- How to choose an AD backend and make it work?

# How AD works

## A multifaceted concept

See @blondelElementsDifferentiableProgramming2024 for a deep dive:

## Numeric differentiation

## Symbolic differentiation

## Algorithmic differentiation
* Let's look at a function $h(x)=g(f(x))$ composed from two differentiable functions $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and $g: \mathbb{R}^m \rightarrow \mathbb{R}^p$.
  $$
  h = g \circ f\
  $$

* The Jacobian of $h$ can be obtained from the Jacobians of $g$ and $f$ using the **chain rule**:

  $$
  J_h\big|_x = J_{g}\big|_{f(x)} \cdot J_{f}\big|_{x}
  $$

* For functions with "deeper" compositional structure, we need to compute the products of many Jacobians.
  * If implemented naively, full Jacobians requires a lot of memory
  * **Instead, let's do this matrix-free using JVPs and VJPs!**


## Forward mode -- Theory
Forward accumulation on a function $f(x) = f^N(\ldots f^2(f^1(x)))$:

```{julia}
#| echo: false
using Kroki # maybe svgbob can be used directly inside revealjs?
svgbob"""
 "x"  â”Œâ”€â”€â”€â”€â”€â” "hâ‚" â”Œâ”€â”€â”€â”€â”€â”"hâ‚‚"    "hâ‚™â‚‹â‚"â”Œâ”€â”€â”€â”€â”€â”  "y"
â”€â”€â”¬â”€â”€â–ºâ”‚ "fÂ¹"â”œâ”€â”€â”¬â”€â”€â–ºâ”‚ "fÂ²"â”œâ”€â”€â”€â–º .. â”€â”€â”¬â”€â”€â–ºâ”‚ "fâ¿"â”œâ”€â”€â”€â”€â”€â”€â”€â–º
  â”‚   â””â”€â”€â”€â”€â”€â”˜  â”‚   â””â”€â”€â”€â”€â”€â”˜          â”‚   â””â”€â”€â”€â”€â”€â”˜
  â”‚   â”Œâ”€â”€â”€â”€â”€â”  â”‚   â”Œâ”€â”€â”€â”€â”€â”          â”‚   â”Œâ”€â”€â”€â”€â”€â”
  â””â”€â”€â–ºâ”‚"ğ’ŸfÂ¹"â”‚  â””â”€â”€â–ºâ”‚"ğ’ŸfÂ²"â”‚          â””â”€â”€â–ºâ”‚"ğ’Ÿfâ¿"â”‚
â”€â”€â”€â”€â”€â–ºâ”‚     â”œâ”€â”€â”€â”€â”€â–ºâ”‚     â”œâ”€â”€â”€â–º .. â”€â”€â”€â”€â”€â–ºâ”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â–º
 "v"  â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”˜"ğ’Ÿfâ‚“(v)"
"""
```

Common names for functions `ğ’Ÿf`:

* Jacobian-vector product (JVP)
* pushforward
* forward-rule (`frule`)

## Forward mode -- Implementation


## Reverse mode -- Theory
Reverse accumulation on a function $f(x) = f^N(\ldots f^2(f^1(x)))$:
```{julia}
#| echo: false
using Kroki # maybe svgbob can be used directly inside revealjs?
svgbob"""
    "x"   â”Œâ”€â”€â”€â”€â”€â”€â” "hâ‚" â”Œâ”€â”€â”€â”€â”€â”€â”"hâ‚‚"    "hâ‚™â‚‹â‚"â”Œâ”€â”€â”€â”€â”€â”€â” "y"
â”€â”€â”€â”€â”€â”€â”¬â”€â”€â–ºâ”‚  "fÂ¹"â”œâ”€â”€â”¬â”€â”€â–ºâ”‚  "fÂ²"â”œâ”€â”€â”€â–º .. â”€â”€â”¬â”€â”€â–ºâ”‚  "fâ¿"â”œâ”€â”€â”€â–º
      â”‚   â””â”€â”€â”€â”€â”€â”€â”˜  â”‚   â””â”€â”€â”€â”€â”€â”€â”˜          â”‚   â””â”€â”€â”€â”€â”€â”€â”˜
      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”          â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”
      â””â”€â”€â–ºâ”‚"ğ’Ÿáµ€fÂ¹"â”‚  â””â”€â”€â–ºâ”‚"ğ’Ÿáµ€fÂ²"â”‚          â””â”€â”€â–ºâ”‚"ğ’Ÿáµ€fâ¿"â”‚
â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚â—„â”€â”€â”€â”€â”€â”¤      â”‚â—„â”€â”€â”€ .. â—„â”€â”€â”€â”€â”€â”¤      â”‚â—„â”€â”€â”€
"ğ’Ÿáµ€fâ‚“(w)" â””â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”˜ "w"
"""
```

Common names for functions `ğ’Ÿáµ€f`:

* Vector-Jacobian product (VJP)
* pullback
* reverse-rule (`rrule`)


## Reverse mode -- Implementation

# Using AD

## Why so many backends?

- Conflicting paradigms:
  - numeric vs. symbolic vs. algorithmic
  - source-to-source vs. operator overloading
  - Julia level vs. LLVM level
- Cover varying subsets of the language
- Historical reasons: developed by different people

## Useful criteria

- Does this backend execute without error?
- Does it return the right derivative?
- Does it run fast enough for me?

## A simple decision tree

1. Follow recommendations of high-level library (e.g. Flux.jl)
2. Otherwise, choose between forward and reverse mode based on input and output dimensions
3. Try out the most battle-tested backends:

:::: {.columns}

::: {.column width="50%"}
Forward mode

- ForwardDiff
- Enzyme
:::

::: {.column width="50%"}
Reverse mode

- Zygote
- Enzyme
:::

4. If nothing works, fall back on finite differences

::::

## DifferentiationInterface

# Enabling AD

## Typical failures

- `Float64` conversion in ForwardDiff
- Mutation in Zygote
- Ugly LLVM vomit in Enzyme

## ForwardDiff: type-genericity

## Zygote: custom rules

## Enzyme: type-stability

## DifferentiationInterfaceTest

# Developing AD

## What's the point?

- Why don't we all use Enzyme?
- Because 3 years ago it was "why don't we all use Diffractor / Zygote?"
- New solutions like Tapir or FastDifferentiation enable different tradeoffs

## DifferentiationInterface & DifferentiationInterfaceTest

- Sometimes, a little performance loss is acceptable for a simpler API
- Systematic test suites catch many errors

# Appendix

## References

::: {#refs}
:::