---
title: "Gradients for everyone"
title-slide-attributes:
  data-background-image: /img/qr/qrbackground.png
  data-background-size: contain
subtitle: "A quick guide to autodiff in Julia"
author:
  - name: Guillaume Dalle
    orcid: 0000-0003-4866-1687
    email: guillaume.dalle@epfl.ch
    affiliation: 
      - name: EPFL
      - department: IdePHICS, INDY & SPOC laboratories
  - name: Adrian Hill
    orcid: 0009-0009-5977-301X
    email: hill@tu-berlin.de
    affiliation: 
      - name: TU Berlin
      - department: Machine Learning group
date: "2024-07-11"
bibliography: AutoDiff.bib
engine: julia
format:
  revealjs:
    theme: [default, custom.scss]
    toc: true
    toc-depth: 1
    slide-number: true
    overview: true
    code-line-numbers: false
    scrollable: true
execute:
    echo: true
    freeze: auto
    error: true
---

# Introduction

## Motivation

::: {.callout-note}
## What is a derivative?

A linear approximation of a function around a point.
:::

::: {.callout-important}
## Why do we care?

Derivatives of computer code are essential in optimization and machine learning.
:::

::: {.callout-tip}
## What do I need to do?

Not much: with Automatic Differentiation (AD), derivatives are easy to compute!
:::

::: {.notes}
- We're more specifically talking about functions expressed as programs! 
:::

## Three types of AD users

1. **Package users** want to differentiate through functions
2. **Package developers** want to write differentiable functions
3. **Backend developers** want to create new AD systems

## Python vs. Julia: user experience

![](img/python_julia_user.png)

::: {.notes}
Imagine we need to use two packages: one for Foo, one for Bar

- In Python, both packages need to be rewritten for each framework
  - (-) "three language problem"
  - (+) compatibility between packages in an ecosystem is almost guaranteed
  - user doesn't need to understand AD at all
- In Julia, only one Foo and Bar package need to exist
  - (+) efforts can be "centralized"
  - (-) compatibility with backends is up to the package developers
    - must be documented
    - hard to understand for newcomers
  - puts burden on user to understand AD
  - ideally, things would "just work"
:::

## Python vs. Julia: developers

![](img/python_julia_dev_1.png)

::: {.notes}
- Let's talk about the developer experience
- In Python, things are simple and boring: JAX, PyTorch or TensorFlow
- In Julia, things are complicated and fun: <https://juliadiff.org/>
:::

## Python vs. Julia: developers

![](img/python_julia_dev_2.png)

::: {.notes}
- Python frameworks come with constraints
  - e.g. JAX doesn't allow inplace mutation
  - but you are aware of these before you start writing your code
- Julia backends come with constraints
  - unless you already had a specific backend in mind when working on your package, you will have to refactor
  - ⚠️ contraints of some backends are "orthogonal" to performant Julia code
    - e.g. no in-place mutation in Zygote
  - A lot of folklore and hidden knowledge, hard to see straight
  - How to choose an AD backend and make it work?
:::

# Understanding AD

## Various flavors of differentiation

- **Manual**: work out $f'$ by hand
- **Numeric**: $f'(x) \approx \frac{f(x+\varepsilon) - f(x)}{\varepsilon}$
- **Symbolic**: code a formula for $f$, get a formula for $f'$
- **Automatic**: code a program for $f$, get a value for $f'(x)$

::: {.notes}
- Manual aka blood, sweat and tears
- Numeric aka finite differences
- Symbolic aka computer algebra
- Automatic aka algorithmic
  - finite differences are also automatic, compute JVPs
:::

## Algorithmic differentiation 

Three key ideas [@griewankEvaluatingDerivativesPrinciples2008]:

1. Programs are **composition** chains (or DAGs) of many functions
2. Jacobian of $f = f_L \circ \dots \circ f_2 \circ f_1$ given by the **chain rule**:
$$
J = J_L J_{L-1} \dots J_2 J_1
$$
3. Avoid materializing full Jacobians with **matrix-vector products**: we only need $Jv$ and $v^\top J$

::: {.notes}
- Most notably for point 3, a matrix-vector-product with the i-th standard basis vector returns the i-th column of the matrix 
:::

## Forward mode

**Jacobian-Vector Products** (JVPs), aka pushforwards, are naturally decomposed from $1$ to $L$:
$$
J v = J_L (J_{L-1}(\dots J_2(J_1 v)))
$$

For $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, the $m \times n$ Jacobian requires $n$ JVPs:
one per input dimension.

::: {.callout-note}
## Special case

The derivative of $f : \mathbb{R} \rightarrow \mathbb{R}^m$ requires just one JVP.
:::

## Reverse mode

**Vector-Jacobian Products** (JVPs), aka pullbacks, are naturally decomposed from $L$ to $1$:
$$
v^\top J = (((v^\top J_L) J_{L-1}) \dots J_2)J_1
$$

For $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, the $m \times n$ Jacobian requires $m$ VJPs: 
one per input dimension.

::: {.callout-note}
## Special case

The gradient of $f : \mathbb{R}^n \rightarrow \mathbb{R}$ requires just one VJP.
:::

## Implementation details

:::: {.columns}

::: {.column width="40%"}
### Forward mode

Forward sweep only.

Often based on dual numbers.

Low memory cost.
:::

::: {.column width="50%"}

### Reverse mode

Forward sweep + reverse sweep.

Often based on tapes.

High memory cost.
:::

::::

::: {.notes}
- Jacobians/JVPs/VJPs are computed around a "point of linearization"
  - in forward-mode, the order of the JVP evaluation is the same as the the forward pass through f
  - in reverse-mode, the order is reversed -> we need tape 
:::

# Using AD

## Why so many backends?

- Conflicting **paradigms**:
  - numeric vs. symbolic vs. algorithmic
  - operator overloading vs.  source-to-source (which source?)
- Cover varying **subsets of the language**
- Historical reasons: developed by **different people**

## Meaningful criteria

- Does this AD backend execute **without error**?
- Does it return the **right derivative**?
- Does it run **fast enough** for me?

## A simple decision tree

1. **Follow recommendations** of high-level library (e.g. Flux).
2. Otherwise, **choose mode** based on input and output dimensions.
3. Try the most **battle-tested** backends: [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl) or [Enzyme](https://github.com/EnzymeAD/Enzyme.jl) in forward mode, [Zygote](https://github.com/FluxML/Zygote.jl) or [Enzyme](https://github.com/EnzymeAD/Enzyme.jl) in reverse mode.
4. If nothing works, fall back on finite differences.

# Enabling AD

## Typical ForwardDiff failure

```{julia}
import ForwardDiff

badcopy(x) = copyto!(zeros(size(x)), x)

ForwardDiff.jacobian(badcopy, ones(2))
```

## ForwardDiff troubleshooting

Allow numbers of [type `Dual`](https://juliadiff.org/ForwardDiff.jl/stable/dev/how_it_works/) in your functions.

```{julia}
goodcopy(x::AbstractArray{<:Real}) = copyto!(zeros(eltype(x), size(x)), x)

ForwardDiff.jacobian(goodcopy, ones(2))
```

## Typical Zygote failure

```{julia}
import Zygote

Zygote.jacobian(badcopy, ones(2))
```

## Zygote troubleshooting

Define a [custom rule](https://juliadiff.org/ChainRulesCore.jl/stable/) with [ChainRulesCore](https://github.com/JuliaDiff/ChainRulesCore.jl):

```{julia}
using ChainRulesCore, LinearAlgebra

badcopy2(x) = badcopy(x)

function ChainRulesCore.rrule(::typeof(badcopy2), x)
    @info "My rule is called"
    y = badcopy2(x)  # primal value
    function badcopy2_pullback(dy)
    @info "My pullback is called"
        df = NoTangent()
        dx = I' * dy # Vector-Jacobian product
        return (df, dx)
    end
    return y, badcopy2_pullback
end

Zygote.jacobian(badcopy2, ones(2))
```

## Typical Enzyme failure 

```{julia}
import Enzyme

Enzyme.autodiff(
  Enzyme.Forward,
  badcopy,
  Enzyme.Active(ones(2))
)
```

## Enzyme troubleshooting

Pay attention to type stability, temporary storage and activity annotations (see the [FAQ](https://enzymead.github.io/Enzyme.jl/stable/faq/)).

```julia
Enzyme.autodiff(
  Enzyme.Forward,
  badcopy,
  Enzyme.Duplicated(ones(2), zeros(2))
)
```

# DifferentiationInterface

## Goals

- [DifferentiationInterface](https://github.com/gdalle/DifferentiationInterface.jl) (DI) offers a **common syntax** for all AD backends^[inspired by [AbstractDifferentiation](https://github.com/JuliaDiff/AbstractDifferentiation.jl)]
- AD users can compare correctness and performance **without reading each documentation**
- AD developers get access to a wider user base

::: {.callout-warning}
## The fine print

DI may be slower than a direct call to the backend's API (mostly with Enzyme).
:::

## Supported packages

<!-- Using columns to fit everything on one slide -->
:::: {.columns}

::: {.column width="50%"}
* [ChainRulesCore](https://github.com/JuliaDiff/ChainRulesCore.jl)
* [Diffractor](https://github.com/JuliaDiff/Diffractor.jl)
* [Enzyme](https://github.com/EnzymeAD/Enzyme.jl)
* [FastDifferentiation](https://github.com/brianguenter/FastDifferentiation.jl)
* [FiniteDiff](https://github.com/JuliaDiff/FiniteDiff.jl)
* [FiniteDifferences](https://github.com/JuliaDiff/FiniteDifferences.jl)
* [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl)
:::

::: {.column width="50%"}
* [PolyesterForwardDiff](https://github.com/JuliaDiff/PolyesterForwardDiff.jl)
* [ReverseDiff](https://github.com/JuliaDiff/ReverseDiff.jl)
* [Symbolics](https://github.com/JuliaSymbolics/Symbolics.jl)
* [Tapir](https://github.com/withbayes/Tapir.jl)
* [Tracker](https://github.com/FluxML/Tracker.jl)
* [Zygote](https://github.com/FluxML/Zygote.jl)
:::

::::

## Getting started with DI

**Step 1:** load the necessary packages
```{julia}
#| output: false
using DifferentiationInterface
import ForwardDiff, Enzyme, Zygote

f(x) = sum(abs2, x)
x = [1.0, 2.0, 3.0, 4.0]
```

**Step 2:** Combine [DI's operators](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/operators/) with a backend from [ADTypes](https://github.com/SciML/ADTypes.jl)

```{julia}
value_and_gradient(f, AutoForwardDiff(), x)
```
```{julia}
value_and_gradient(f, AutoEnzyme(), x)
```
```{julia}
value_and_gradient(f, AutoZygote(), x)
```

**Step 3:** Increase performance via [DI's preparation mechanism](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/tutorial1/)

## Features of DI

- Support for functions `f(x)` or `f!(y, x)` with **scalar/array inputs & outputs**
- Eight standard **operators**: `pushforward`, `pullback`, `derivative`, `gradient`, `jacobian`, `hvp`, `second_derivative`, `hessian`
- Out-of-place and **in-place** versions
- **Combine** different backends using [`SecondOrder`](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/api/#DifferentiationInterface.SecondOrder)
- **Translate** between backends using [`DifferentiateWith`](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/api/#DifferentiationInterface.DifferentiateWith)

## DifferentiationInterfaceTest

- **Systematic tests** for a variety of inputs and functions
- Scenarios with **weird arrays** (static, GPU, sparse)
- Type-stability checks
- Automated **benchmarks**

## Sparse AD ecosystem

- [SparseConnectivityTracer](https://github.com/adrhill/SparseConnectivityTracer.jl) for sparsity pattern detection
- [SparseMatrixColorings](https://github.com/gdalle/SparseMatrixColorings.jl) for matrix coloring
- DI for sparse Jacobians and Hessians (see [tutorial](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/tutorial2/))

![Sparse AD with coloring [@gebremedhinWhatColorYour2005]](img/coloring.png)

# Conclusion

## What's next?

DI and its sparse AD ecosystem are brand new projects:

- Try them out in your code
- Report bugs or inefficiencies
- Help us improve these packages!

::: {.callout-note}
## Coming soon in DI (JuliaCon hackathon?)

Support for multiple arguments and non-array types.
:::

## More complex settings

- [x] AD through a simple function
- [ ] AD through an integral?
- [ ] AD through an optimization solver?
- [ ] AD through a stochastic expectation?

More details in the book by @blondelElementsDifferentiableProgramming2024.

## Take-home message

**Computing derivatives is easy**, but each AD solution comes with its own **limitations**.

Learn to recognize and overcome them, either as a user or as a developer.

## References

::: {#refs}
:::