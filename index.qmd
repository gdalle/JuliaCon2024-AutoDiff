---
title: "Gradients for everyone"
subtitle: "A quick guide to autodiff in Julia"
author:
  - name: Guillaume Dalle
    orcid: 0000-0003-4866-1687
    email: guillaume.dalle@epfl.ch
    affiliation: 
      - name: EPFL
      - department: IdePHICS, INDY & SPOC laboratories
  - name: Adrian Hill
    orcid: 0009-0009-5977-301X
    email: hill@tu-berlin.de
    affiliation: 
      - name: TU Berlin
      - department: Machine Learning group
date: "2024-07-11"
bibliography: AutoDiff.bib
engine: julia
format:
  revealjs:
    theme: [default, custom.scss]
    toc: true
    toc-depth: 1
    slide-number: true
    overview: true
    code-line-numbers: false
execute:
    echo: true
---


## Follow along
<!-- Ideally, we figure out how to put this on the title slide -->
Slides available on GitHub: 

![](img/qr/qrcode.png)

[github.com/gdalle/JuliaCon2024-AutoDiff](https://github.com/gdalle/JuliaCon2024-AutoDiff)

# Introduction

## Motivation

- What is a derivative?
- Why do we care?

## Differentiable programming

- A function is not just math
- Blocks of code are functions too

## Three types of AD users

1. **Package users** want to differentiate through functions
2. **Package developers** want to write differentiable functions
3. **Backend developers** want to create new AD systems

## Python vs. Julia

- In Python, things are simple and boring: JAX, PyTorch or TensorFlow
- In Julia, things are complicated and fun: <https://juliadiff.org/>
- A lot of folklore and hidden knowledge, hard to see straight
- How to choose an AD backend and make it work?

# How AD works

## A multifaceted concept

See @blondelElementsDifferentiableProgramming2024 for a deep dive:

## Numeric differentiation

## Symbolic differentiation

## Algorithmic differentiation  {.smaller}
* Let's look at a function $h(x)=g(f(x))$ composed from two differentiable functions $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and $g: \mathbb{R}^m \rightarrow \mathbb{R}^p$
  $$
  h = g \circ f \quad .
  $$

* The Jacobian of $h$ can be obtained from the Jacobians of $g$ and $f$ using the **chain rule**:

  $$
  J_h\big|_x = J_{g}\big|_{f(x)} \cdot J_{f}\big|_{x}
  $$

* For functions with "deeper" compositional structure, we need to compute the products of many Jacobians.
  * If implemented naively, full Jacobians require a lot of memory
  * **Instead, let's do this matrix-free using JVPs and VJPs!**

## Jacobian-vector products {.smaller}

* Also called **JVPs**, **pushforwards** or **forward rules**
* Multiplying Jacobian with $i$-th column basis vector $e_i$ computes $i$-th column of Jacobian 
  $$
  \begin{equation}
  J_f\big|_x \cdot e_i
  = \begin{bmatrix}
      \dfrac{\partial f_1}{\partial x_1}\Bigg|_x & \cdots &
    \dfrac{\partial f_1}{\partial x_n}\Bigg|_x\\
      \vdots                             & \ddots & \vdots\\
      \dfrac{\partial f_m}{\partial x_1}\Bigg|_x & \cdots &
    \dfrac{\partial f_m}{\partial x_n}\Bigg|_x
  \end{bmatrix} \cdot e_i
  = \begin{bmatrix}
      \dfrac{\partial f_1}{\partial x_i}\Bigg|_x \\
      \vdots \\
      \dfrac{\partial f_m}{\partial x_i}\Bigg|_x \\
  \end{bmatrix}
  =: \mathcal{D}f_x(e_i)
  \end{equation}
  $$

* For $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, computing the full $m \times n$ Jacobian requires computing $n\,$ JVPs:  
  one for each column, as many as the **input dimensionality** of $f$.

## Vector-Jacobian products {.smaller}

* Also called **VJPs**, **pullbacks** or **reverse rules**
* Multiplying $i$-th row basis vector $e_i^T$ with Jacobian computes $i$-th row of Jacobian 
  $$
  \begin{align}
  e_i^T \cdot J_f\big|_x
  &= e_i^T \cdot \begin{bmatrix}
      \dfrac{\partial f_1}{\partial x_1}\Bigg|_x & \cdots &
    \dfrac{\partial f_1}{\partial x_n}\Bigg|_x\\
      \vdots                             & \ddots & \vdots\\
      \dfrac{\partial f_m}{\partial x_1}\Bigg|_x & \cdots &
    \dfrac{\partial f_m}{\partial x_n}\Bigg|_x
  \end{bmatrix} \\[0.5em]
  &= \hspace{1.35em}\begin{bmatrix}
      \dfrac{\partial f_i}{\partial x_1}\Bigg|_x & \cdots &
    \dfrac{\partial f_i}{\partial x_n}\Bigg|_x
  \end{bmatrix}
  =: \mathcal{D}^{*}f_x(e_i)
  \end{align}
  $$

* Computing the full $m \times n$ Jacobian requires computing $m\,$ VJPs: 
  one for each row, as many as the **output dimensionality** of $f$.

## Forward mode -- Theory {.smaller}
* Assuming a function $f(x) = f^N(\ldots f^2(f^1(x)))$ with intermediate activations
  $$
  \begin{equation}
    h_1 = f^1(x) ,\quad 
    h_2 = f^2(f^1(x)) ,\quad
    h_k = f^{k}(h_{k-1})
  \end{equation} \quad ,
  $$

  we can compute JVPs of $f$ by **forward-accumulating** JVPs of $f^i$: 
  $$
  % parentheses for emphasis
  \begin{equation}
  J_f\big|_x \cdot v
  = J_{f^{N  }}\big|_{h_{N-1}} \cdot
    \Big( \ldots \cdot
    \Big( J_{f^{2  }}\big|_{h_{1  }} \cdot
    \Big( J_{f^{1  }}\big|_{x} \cdot v \Big)\Big)\Big)
  \end{equation}
  $$

* or in terms of **pushforwards**:
  $$
  \begin{equation}
  \mathcal{D}f_x(v)
  = \big(
  \mathcal{D}f^{N}_{h_{N-1}} \circ \ldots \circ
	\mathcal{D}f^2_{h_1}  \circ
	\mathcal{D}f^1_{x}
  \big)(v)  
  \end{equation}
  $$
  
```{julia}
#| echo: false
using Kroki # maybe svgbob can be used directly inside revealjs?
svgbob"""
 "x"  ┌─────┐ "h₁" ┌─────┐"h₂"    "hₙ₋₁"┌─────┐  "y"
──┬──►│ "f¹"├──┬──►│ "f²"├───► .. ──┬──►│ "fᴺ"├───────►
  │   └─────┘  │   └─────┘          │   └─────┘
  │   ┌─────┐  │   ┌─────┐          │   ┌─────┐
  └──►│"𝒟f¹"│  └──►│"𝒟f²"│          └──►│"𝒟fᴺ"│
─────►│     ├─────►│     ├───► .. ─────►│     ├───────►
 "v"  └─────┘      └─────┘              └─────┘"𝒟fₓ(v)"
"""
```

## Forward mode -- Implementation

## Reverse mode -- Theory {.smaller}
* we can compute VJPs of $f$ by **reverse-accumulating** VJPs of $f^i$: 
  $$
  % parentheses for emphasis
  \begin{equation}
  w^T \cdot J_f\big|_x
  = \Big(\Big(\Big( w^T \cdot
    J_{f^{N  }}\big|_{h_{N-1}} \Big) \cdot
    J_{f^{N-1}}\big|_{h_{N-2}} \Big) \cdot \ldots \Big)  \cdot
    J_{f^{1  }}\big|_x
  \end{equation}
  $$

* or in terms of **pullbacks**:
  $$
  \begin{equation}
  \mathcal{D}^{*}f_x(w)
  = \big(
    \mathcal{D}^{*}f^{1  }_x  \circ \ldots \circ
    \mathcal{D}^{*}f^{N-1}_{h_{N-2}} \circ
    \mathcal{D}^{*}f^{N  }_{h_{N-1}} 
  \big) (w) \quad .
  \end{equation}
  $$

```{julia}
#| echo: false
using Kroki # maybe svgbob can be used directly inside revealjs?
svgbob"""
    "x"   ┌──────┐ "h₁" ┌──────┐"h₂"    "hₙ₋₁"┌──────┐ "y"
──────┬──►│  "f¹"├──┬──►│  "f²"├───► .. ──┬──►│  "fᴺ"├───►
      │   └──────┘  │   └──────┘          │   └──────┘
      │   ┌──────┐  │   ┌──────┐          │   ┌──────┐
      └──►│"𝒟*f¹"│  └──►│"𝒟*f²"│          └──►│"𝒟*fᴺ"│
◄─────────┤      │◄─────┤      │◄─── .. ◄─────┤      │◄───
"𝒟*fₓ(w)" └──────┘      └──────┘              └──────┘ "w"
"""
```

## Reverse mode -- Implementation

# Using AD

## Why so many backends?

- Conflicting paradigms:
  - numeric vs. symbolic vs. algorithmic
  - source-to-source vs. operator overloading
  - Julia level vs. LLVM level
- Cover varying subsets of the language
- Historical reasons: developed by different people

## Useful criteria

- Does this backend execute without error?
- Does it return the right derivative?
- Does it run fast enough for me?

## A simple decision tree

1. Follow recommendations of high-level library (e.g. Flux.jl)
2. Otherwise, choose between forward and reverse mode based on input and output dimensions
3. Try out the most battle-tested backends:

:::: {.columns}

::: {.column width="50%"}
Forward mode

- ForwardDiff
- Enzyme
:::

::: {.column width="50%"}
Reverse mode

- Zygote
- Enzyme
:::

4. If nothing works, fall back on finite differences

::::

# DifferentiationInterface.jl

## Supported backends

<!-- Using columns to fit everything on one slide -->
:::: {.columns}

::: {.column width="50%"}
* [ChainRulesCore.jl](https://github.com/JuliaDiff/ChainRulesCore.jl)
* [Diffractor.jl](https://github.com/JuliaDiff/Diffractor.jl)
* [Enzyme.jl](https://github.com/EnzymeAD/Enzyme.jl)
* [FastDifferentiation.jl](https://github.com/brianguenter/FastDifferentiation.jl)
* [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl)
* [FiniteDifferences.jl](https://github.com/JuliaDiff/FiniteDifferences.jl)
* [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)
:::

::: {.column width="50%"}
* [PolyesterForwardDiff.jl](https://github.com/JuliaDiff/PolyesterForwardDiff.jl)
* [ReverseDiff.jl](https://github.com/JuliaDiff/ReverseDiff.jl)
* [Symbolics.jl](https://github.com/JuliaSymbolics/Symbolics.jl)
* [Tapir.jl](https://github.com/withbayes/Tapir.jl)
* [Tracker.jl](https://github.com/FluxML/Tracker.jl)
* [Zygote.jl](https://github.com/FluxML/Zygote.jl)
:::

::::

Using [ADTypes.jl](https://github.com/SciML/ADTypes.jl) to select backends.

## Getting started with DI

1. `import` desired backends
```{julia}
#| output: false
using DifferentiationInterface
import ForwardDiff, Enzyme, Zygote

f(x) = sum(abs2, x)
x = [1.0, 2.0, 3.0, 4.0]
```

2. Call [DI's operators](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/operators/) using [ADTypes](https://github.com/SciML/ADTypes.jl)
```{julia}
value_and_gradient(f, AutoForwardDiff(), x)
```

```{julia}
value_and_gradient(f, AutoEnzyme(), x)
```

```{julia}
value_and_gradient(f, AutoZygote(), x)
```

3. Increase performance via [DI's preparation mechanism](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/tutorial1/)

# Enabling AD

## Typical failures

- `Float64` conversion in ForwardDiff
- Mutation in Zygote
- Ugly LLVM vomit in Enzyme

## ForwardDiff: type-genericity

## Zygote: custom rules

## Enzyme: type-stability

## DifferentiationInterfaceTest.jl

# Developing AD

## What's the point?

- Why don't we all use Enzyme?
- Because 3 years ago it was "why don't we all use Diffractor / Zygote?"
- New solutions like Tapir or FastDifferentiation enable different tradeoffs

## DifferentiationInterface.jl & DifferentiationInterfaceTest.jl

- Sometimes, a little performance loss is acceptable for a simpler API
- Systematic test suites catch many errors

# Appendix

## References

::: {#refs}
:::