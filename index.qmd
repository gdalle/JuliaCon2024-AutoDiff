---
title: "Gradients for everyone"
subtitle: "A quick guide to autodiff in Julia"
author:
  - name: Guillaume Dalle
    orcid: 0000-0003-4866-1687
    email: guillaume.dalle@epfl.ch
    affiliation: 
      - name: EPFL
      - department: IdePHICS, INDY & SPOC laboratories
  - name: Adrian Hill
    orcid: 0009-0009-5977-301X
    email: hill@tu-berlin.de
    affiliation: 
      - name: TU Berlin
      - department: Machine Learning group
date: "2024-07-11"
bibliography: AutoDiff.bib
engine: julia
format:
  revealjs:
    theme: [default, custom.scss]
    toc: true
    toc-depth: 1
    slide-number: true
    overview: true
    code-line-numbers: false
execute:
    echo: true
    freeze: auto
---

## Follow along

::: {.qr-code}
![](img/qr/qrcode.png)

[github.com/gdalle/JuliaCon2024-AutoDiff](https://github.com/gdalle/JuliaCon2024-AutoDiff)
:::

# Introduction

## Motivation

::: {.callout-note}
## What is a derivative?

A linear approximation of a function around a point.
:::

::: {.callout-important}
## Why do we care?

Derivatives are essential in optimization and machine learning.
:::

::: {.callout-tip}
## What do I need to do?

Not much: with Automatic Differentiation (AD), derivatives are very easy to compute!
:::

## Differentiable programming

> Differentiable programming is a programming paradigm in which **complex computer programs** (including those with control flows and data structures) can be **differentiated end-to-end automatically**, enabling gradient-based optimization of parameters in the program. In differentiable programming, a program is also defined as the composition of elementary operations, forming a **computation graph**.

From the book by @blondelElementsDifferentiableProgramming2024

## Three types of AD users

1. **Package users** want to differentiate through functions
2. **Package developers** want to write differentiable functions
3. **Backend developers** want to create new AD systems

## Python vs. Julia: users

![](img/python_julia_user.png)

::: {.notes}
Imagine we need to use two packages: one for Foo, one for Bar

- In Python, both packages need to be rewritten for each framework
  - (-) "three language problem"
  - (+) compatibility between packages in an ecosystem is almost guaranteed
- In Julia, only one Foo and Bar package need to exist
  - (+) efforts can be "centralized"
  - (-) compatibility with backends is up to the package developers
    - must be documented
    - hard to understand for newcomers
:::

## Python vs. Julia: developers

![](img/python_julia_dev.png)

::: {.notes}
- In Python, things are simple and boring: JAX, PyTorch or TensorFlow
- In Julia, things are complicated and fun: <https://juliadiff.org/>
- A lot of folklore and hidden knowledge, hard to see straight
- How to choose an AD backend and make it work?
:::

# How AD works

## Various flavors of differentiation

- **Manual**: work out $f'$ by hand
- **Numeric**: $f'(x) \approx \frac{f(x+\varepsilon) - f(x)}{\varepsilon}$
- **Symbolic**: code a formula for $f$, get a formula for $f'$
- **Algorithmic**: code a program for $f$, get a value for $f'(x)$


::: {.notes}
- Manual aka blood, sweat and tears
- Numeric aka finite differences
- Symbolic aka computer algebra
- Algorithmic aka autodiff
:::


## Algorithmic differentiation 

Three key ideas:

1. Program = composition chain (or DAG) of many functions
2. Jacobian of $f = f_L \circ \dots \circ f_2 \circ f_1$ given by the chain rule:
$$
J = J_L J_{L-1} \dots J_2 J_1
$$
1. Avoid materializing full Jacobians with matrix-vector products

## Forward mode

Natural decomposition of **Jacobian-Vector Products** (JVPs), aka pushforwards or forward rules:
$$
J v = J_L (J_{L-1}(\dots J_2(J_1 v)))
$$

For $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, the $m \times n$ Jacobian requires $n$ JVPs:
one per input dimension.

::: {.callout-note}
## Special case

The derivative of $f : \mathbb{R} \rightarrow \mathbb{R}^m$ requires just one JVP.
:::

## Reverse mode

Natural decomposition of **Vector-Jacobian Products** (JVPs), aka pullbacks or reverse rules:
$$
v^\top J = (((v^\top J_L) J_{L-1}) \dots J_2)J_1
$$

For $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$, the $m \times n$ Jacobian requires $m$ VJPs: 
one per input dimension.

::: {.callout-note}
## Special case

The gradient of $f : \mathbb{R}^n \rightarrow \mathbb{R}$ requires just one JVP.
:::

## Implementation details

:::: {.columns}

::: {.column width="45%"}
### Forward mode

Forward sweep only.

Based on dual numbers.

Low memory cost.

![](img/forward.png){width="100%"}
:::

::: {.column width="55%"}

### Reverse mode

Forward sweep + reverse sweep.

Based on tapes.

High memory cost.

![](img/reverse.png){width="100%"}
:::

::::

Images from @blondelElementsDifferentiableProgramming2024

# Using AD

## Why so many backends?

- Conflicting paradigms:
  - numeric vs. symbolic vs. algorithmic
  - operator overloading vs.  source-to-source (which source?)
- Cover varying subsets of the language
- Historical reasons: developed by different people

## Meaningful criteria

- Does this AD backend execute without error?
- Does it return the right derivative?
- Does it run fast enough for me?

## A simple decision tree

1. Follow recommendations of high-level library (e.g. Flux)
2. Choose mode based on input and output dimensions
3. Try the most battle-tested backends:
  - in forward mode: [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl) or [Enzyme](https://github.com/EnzymeAD/Enzyme.jl)
  - in reverse mode: [Zygote](https://github.com/FluxML/Zygote.jl) or [Enzyme](https://github.com/EnzymeAD/Enzyme.jl)
4. If nothing works, fall back on finite differences

# Enabling AD

## Typical failures

- `Float64` conversion in ForwardDiff
- Mutation in Zygote
- Ugly LLVM vomit in Enzyme

## ForwardDiff: type-genericity

ForwardDiff propagates numbers of [type `Dual`](https://juliadiff.org/ForwardDiff.jl/stable/dev/how_it_works/).

| replace this | with that |
|---|---|
| `f(x::Vector{Float64})` | `f(x::Vector{<:Real})` |
| `zeros(n)` | `zeros(eltype(x), n)` |

## Zygote: custom rules

If Zygote fails or is too slow, define a [custom rule](https://juliadiff.org/ChainRulesCore.jl/stable/) with [ChainRulesCore](https://github.com/JuliaDiff/ChainRulesCore.jl):

```julia
using ChainRulesCore

function ChainRulesCore.rrule(::typeof{myfunc}, x)
    y = myfunc(x)
    function myfunc_pullback(dy)
        df = NoTangent()
        dx = J'dy # Vector-Jacobian product
        return (df, dx)
    end
    return y, myfunc_pullback
end
```

The rule returns the primal value and a function for the VJP.

## Enzyme: type-stability and activities

Enzyme should be able to differentiate most code, but if not:

- Improve type stability
- Check that you provide temporary storage (with `Duplicated`) when necessary
- Read the [FAQ](https://enzymead.github.io/Enzyme.jl/stable/faq/)

# DifferentiationInterface

## Goals

- [DifferentiationInterface](https://github.com/gdalle/DifferentiationInterface.jl) (DI) offers a common syntax for all AD backends^[inspired by [AbstractDifferentiation](https://github.com/JuliaDiff/AbstractDifferentiation.jl)]
- AD users can compare correctness and performance without reading each documentation
- AD developers get access to a wider user base

::: {.callout-warning}
## The fine print

DI may be slower than a direct call to the backend's API (mostly with Enzyme).
:::

## Supported packages

<!-- Using columns to fit everything on one slide -->
:::: {.columns}

::: {.column width="50%"}
* [ChainRulesCore](https://github.com/JuliaDiff/ChainRulesCore.jl)
* [Diffractor](https://github.com/JuliaDiff/Diffractor.jl)
* [Enzyme](https://github.com/EnzymeAD/Enzyme.jl)
* [FastDifferentiation](https://github.com/brianguenter/FastDifferentiation.jl)
* [FiniteDiff](https://github.com/JuliaDiff/FiniteDiff.jl)
* [FiniteDifferences](https://github.com/JuliaDiff/FiniteDifferences.jl)
* [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl)
:::

::: {.column width="50%"}
* [PolyesterForwardDiff](https://github.com/JuliaDiff/PolyesterForwardDiff.jl)
* [ReverseDiff](https://github.com/JuliaDiff/ReverseDiff.jl)
* [Symbolics](https://github.com/JuliaSymbolics/Symbolics.jl)
* [Tapir](https://github.com/withbayes/Tapir.jl)
* [Tracker](https://github.com/FluxML/Tracker.jl)
* [Zygote](https://github.com/FluxML/Zygote.jl)
:::

::::

## Getting started with DI

**Step 1:** load the necessary packages
```{julia}
#| output: false
using DifferentiationInterface
import ForwardDiff, Enzyme, Zygote

f(x) = sum(abs2, x)
x = [1.0, 2.0, 3.0, 4.0]
```

**Step 2:** Combine [DI's operators](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/operators/) with a backend from [ADTypes](https://github.com/SciML/ADTypes.jl)

```{julia}
value_and_gradient(f, AutoForwardDiff(), x)
```
```{julia}
value_and_gradient(f, AutoEnzyme(), x)
```
```{julia}
value_and_gradient(f, AutoZygote(), x)
```

**Step 3:** Increase performance via [DI's preparation mechanism](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/tutorial1/)

## DifferentiationInterfaceTest

- Systematic tests for a variety of inputs and functions
- Scenarios with weird arrays (static, GPU, sparse)
- Type-stability checks
- Automated benchmarks

## Sparse AD ecosystem

- [SparseConnectivityTracer](https://github.com/adrhill/SparseConnectivityTracer.jl) for sparsity pattern detection
- [SparseMatrixColorings](https://github.com/gdalle/SparseMatrixColorings.jl) for matrix coloring
- DI for sparse Jacobians and Hessians (see [tutorial](https://gdalle.github.io/DifferentiationInterface.jl/DifferentiationInterface/stable/tutorial2/))

![Sparse AD with coloring [@gebremedhinWhatColorYour2005]](img/coloring.png)

## What's next?

DI and its sparse AD ecosystem are brand new projects:

- Try them out in your code
- Report bugs or inefficiencies
- Help us improve these packages!

::: {.callout-note}
## Coming up in DI

Multiple argument and non-array support.
:::

## Take-home message

Computing derivatives in Julia is easy, but each AD backend comes with its own limitations.

Learn to recognize and overcome them, either as a user or as a developer.

# Appendix

## References

::: {#refs}
:::