[
  {
    "objectID": "ideas.html",
    "href": "ideas.html",
    "title": "JuliaCon talk on AD",
    "section": "",
    "text": "The state of automatic differentiation in Julia\nAutodiff in Julia from the user perspective\nA quick guide to autodiff in Julia\nA quick tour of AD in Julia\nNavigating the Julia AD ecosystem\nAD in Julia: A Beginner’s Guide to Backends and Beyond\nFrom Basics to Backends: A Complete Guide to Automatic Differentiation in Julia\n\n\n\n\n\nComplexity of the Julia ecosystem\n\nNumber of backends can be overwhelming / confusing (just open https://juliadiff.org/)\n\nDevs: Which backend do you use?\nUsers: Which function/method is differentiable with which backend?\n\nLots of folklore in Julia community\n\nA quick taxonomy of AD systems\n\nWe want gradients and more generally Jacobians\nAD systems only compute VJPs and JVPs\n\nFor function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\)\n\n\\(n\\) JVPs to compute Jacobian\n\\(m\\) VJPs to compute Jacobian (\\(m=1\\) for e.g. neural networks with scalar loss)\n\nVJPs and JVPs compose really really well due to the chain rule\nwe are not actually allocating potential huge Jacobian matrices but working with functions\n\nfunctions computing JVPs: pushforwards\nfunctions computing VJPs: pullbacks\n\ngood way to broadly categorize AD systems\n\nthere are other exotic approaches\n\nStructure of Autodiff in Julia (compared to e.g. Python) “Julia Dreams Big”\n\nAmbitious goal: language-wide AD\n\nPackages use different AD “backends”\n\nPyTorch and JAX: sub-ecosystems\n\nAD “backends” have their own ecosystems\n\n\nReverse-mode’s “two language problem”\n\nDifferent ways to write Julia code (see “1.5 language problem”)\nIn autodiff it’s reversed (Zygote doesn’t support mutation)\n\nUsing Julia AD in 2024\n\nThere are three types of users:\n\npeople developing AD systems\npeople who want to make their functions differentiable\npeople who want to differentiate over a function\n\nAD developers\nMaking functions differentiable\n\nProbably want compatibility with as many backends as possible\nHow do you do this with ChainRules, Enzyme, ForwardDiff while being fast?\nIs it even possible to have code differentiable across all backends?\n\nDifferentiating over a function\n\nWhich backend is compatible with my problem?\nWhich backend is the fastest?\n\n\nThe solutions (?)\n\nReverse-diff over mutating code being solved by Enzyme\n\nWhy don’t we all just use Enzyme? Complicated interface\n\nAbstractDiff / Diff’Interface\n\nAbstract API allows quickly testing all backends and benchmarking them against each other\n\n\n\n\n\n\n\nhttps://adrianhill.de/julia-ml-course/L6_Automatic_Differentiation/\nhttps://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html"
  },
  {
    "objectID": "ideas.html#title",
    "href": "ideas.html#title",
    "title": "JuliaCon talk on AD",
    "section": "",
    "text": "The state of automatic differentiation in Julia\nAutodiff in Julia from the user perspective\nA quick guide to autodiff in Julia\nA quick tour of AD in Julia\nNavigating the Julia AD ecosystem\nAD in Julia: A Beginner’s Guide to Backends and Beyond\nFrom Basics to Backends: A Complete Guide to Automatic Differentiation in Julia"
  },
  {
    "objectID": "ideas.html#draft-outline",
    "href": "ideas.html#draft-outline",
    "title": "JuliaCon talk on AD",
    "section": "",
    "text": "Complexity of the Julia ecosystem\n\nNumber of backends can be overwhelming / confusing (just open https://juliadiff.org/)\n\nDevs: Which backend do you use?\nUsers: Which function/method is differentiable with which backend?\n\nLots of folklore in Julia community\n\nA quick taxonomy of AD systems\n\nWe want gradients and more generally Jacobians\nAD systems only compute VJPs and JVPs\n\nFor function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\)\n\n\\(n\\) JVPs to compute Jacobian\n\\(m\\) VJPs to compute Jacobian (\\(m=1\\) for e.g. neural networks with scalar loss)\n\nVJPs and JVPs compose really really well due to the chain rule\nwe are not actually allocating potential huge Jacobian matrices but working with functions\n\nfunctions computing JVPs: pushforwards\nfunctions computing VJPs: pullbacks\n\ngood way to broadly categorize AD systems\n\nthere are other exotic approaches\n\nStructure of Autodiff in Julia (compared to e.g. Python) “Julia Dreams Big”\n\nAmbitious goal: language-wide AD\n\nPackages use different AD “backends”\n\nPyTorch and JAX: sub-ecosystems\n\nAD “backends” have their own ecosystems\n\n\nReverse-mode’s “two language problem”\n\nDifferent ways to write Julia code (see “1.5 language problem”)\nIn autodiff it’s reversed (Zygote doesn’t support mutation)\n\nUsing Julia AD in 2024\n\nThere are three types of users:\n\npeople developing AD systems\npeople who want to make their functions differentiable\npeople who want to differentiate over a function\n\nAD developers\nMaking functions differentiable\n\nProbably want compatibility with as many backends as possible\nHow do you do this with ChainRules, Enzyme, ForwardDiff while being fast?\nIs it even possible to have code differentiable across all backends?\n\nDifferentiating over a function\n\nWhich backend is compatible with my problem?\nWhich backend is the fastest?\n\n\nThe solutions (?)\n\nReverse-diff over mutating code being solved by Enzyme\n\nWhy don’t we all just use Enzyme? Complicated interface\n\nAbstractDiff / Diff’Interface\n\nAbstract API allows quickly testing all backends and benchmarking them against each other"
  },
  {
    "objectID": "ideas.html#sources",
    "href": "ideas.html#sources",
    "title": "JuliaCon talk on AD",
    "section": "",
    "text": "https://adrianhill.de/julia-ml-course/L6_Automatic_Differentiation/\nhttps://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html"
  },
  {
    "objectID": "index.html#follow-along",
    "href": "index.html#follow-along",
    "title": "Gradients for everyone",
    "section": "Follow along",
    "text": "Follow along\n\n\ngithub.com/gdalle/JuliaCon2024-AutoDiff"
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Gradients for everyone",
    "section": "Motivation",
    "text": "Motivation\n\n\n\n\n\n\nWhat is a derivative?\n\n\nA linear approximation of a function around a point.\n\n\n\n\n\n\n\n\n\nWhy do we care?\n\n\nDerivatives are essential in optimization and machine learning.\n\n\n\n\n\n\n\n\n\nWhat do I need to do?\n\n\nNot much: with Automatic Differentiation (AD), derivatives are very easy to compute!"
  },
  {
    "objectID": "index.html#differentiable-programming",
    "href": "index.html#differentiable-programming",
    "title": "Gradients for everyone",
    "section": "Differentiable programming",
    "text": "Differentiable programming\n\nDifferentiable programming is a programming paradigm in which complex computer programs (including those with control flows and data structures) can be differentiated end-to-end automatically, enabling gradient-based optimization of parameters in the program. In differentiable programming, a program is also defined as the composition of elementary operations, forming a computation graph.\n\nFrom the book by Blondel and Roulet (2024)"
  },
  {
    "objectID": "index.html#three-types-of-ad-users",
    "href": "index.html#three-types-of-ad-users",
    "title": "Gradients for everyone",
    "section": "Three types of AD users",
    "text": "Three types of AD users\n\nPackage users want to differentiate through functions\nPackage developers want to write differentiable functions\nBackend developers want to create new AD systems"
  },
  {
    "objectID": "index.html#python-vs.-julia-users",
    "href": "index.html#python-vs.-julia-users",
    "title": "Gradients for everyone",
    "section": "Python vs. Julia: users",
    "text": "Python vs. Julia: users\n\n\nImagine we need to use two packages: one for Foo, one for Bar\n\nIn Python, both packages need to be rewritten for each framework\n\n(-) “three language problem”\n(+) compatibility between packages in an ecosystem is almost guaranteed\n\nIn Julia, only one Foo and Bar package need to exist\n\n(+) efforts can be “centralized”\n(-) compatibility with backends is up to the package developers\n\nmust be documented\nhard to understand for newcomers"
  },
  {
    "objectID": "index.html#python-vs.-julia-developers",
    "href": "index.html#python-vs.-julia-developers",
    "title": "Gradients for everyone",
    "section": "Python vs. Julia: developers",
    "text": "Python vs. Julia: developers\n\n\n\nIn Python, things are simple and boring: JAX, PyTorch or TensorFlow\nIn Julia, things are complicated and fun: https://juliadiff.org/\nA lot of folklore and hidden knowledge, hard to see straight\nHow to choose an AD backend and make it work?"
  },
  {
    "objectID": "index.html#various-flavors-of-differentiation",
    "href": "index.html#various-flavors-of-differentiation",
    "title": "Gradients for everyone",
    "section": "Various flavors of differentiation",
    "text": "Various flavors of differentiation\n\nManual: work out \\(f'\\) by hand\nNumeric: \\(f'(x) \\approx \\frac{f(x+\\varepsilon) - f(x)}{\\varepsilon}\\)\nSymbolic: code a formula for \\(f\\), get a formula for \\(f'\\)\nAlgorithmic: code a program for \\(f\\), get a value for \\(f'(x)\\)\n\n\n\nManual aka blood, sweat and tears\nNumeric aka finite differences\nSymbolic aka computer algebra\nAlgorithmic aka autodiff"
  },
  {
    "objectID": "index.html#algorithmic-differentiation",
    "href": "index.html#algorithmic-differentiation",
    "title": "Gradients for everyone",
    "section": "Algorithmic differentiation",
    "text": "Algorithmic differentiation\nThree key ideas:\n\nProgram = composition chain (or DAG) of many functions\nJacobian of \\(f = f_L \\circ \\dots \\circ f_2 \\circ f_1\\) given by the chain rule: \\[\nJ = J_L J_{L-1} \\dots J_2 J_1\n\\]\nAvoid materializing full Jacobians with matrix-vector products"
  },
  {
    "objectID": "index.html#forward-mode",
    "href": "index.html#forward-mode",
    "title": "Gradients for everyone",
    "section": "Forward mode",
    "text": "Forward mode\nNatural decomposition of Jacobian-Vector Products (JVPs), aka pushforwards or forward rules: \\[\nJ v = J_L (J_{L-1}(\\dots J_2(J_1 v)))\n\\]\nFor \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), the \\(m \\times n\\) Jacobian requires \\(n\\) JVPs: one per input dimension.\n\n\n\n\n\n\nSpecial case\n\n\nThe derivative of \\(f : \\mathbb{R} \\rightarrow \\mathbb{R}^m\\) requires just one JVP."
  },
  {
    "objectID": "index.html#reverse-mode",
    "href": "index.html#reverse-mode",
    "title": "Gradients for everyone",
    "section": "Reverse mode",
    "text": "Reverse mode\nNatural decomposition of Vector-Jacobian Products (JVPs), aka pullbacks or reverse rules: \\[\nv^\\top J = (((v^\\top J_L) J_{L-1}) \\dots J_2)J_1\n\\]\nFor \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), the \\(m \\times n\\) Jacobian requires \\(m\\) VJPs: one per input dimension.\n\n\n\n\n\n\nSpecial case\n\n\nThe gradient of \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) requires just one JVP."
  },
  {
    "objectID": "index.html#implementation-details",
    "href": "index.html#implementation-details",
    "title": "Gradients for everyone",
    "section": "Implementation details",
    "text": "Implementation details\n\n\nForward mode\nForward sweep only.\nBased on dual numbers.\nLow memory cost.\n\n\nReverse mode\nForward sweep + reverse sweep.\nBased on tapes.\nHigh memory cost.\n\n\nImages from Blondel and Roulet (2024)"
  },
  {
    "objectID": "index.html#why-so-many-backends",
    "href": "index.html#why-so-many-backends",
    "title": "Gradients for everyone",
    "section": "Why so many backends?",
    "text": "Why so many backends?\n\nConflicting paradigms:\n\nnumeric vs. symbolic vs. algorithmic\noperator overloading vs. source-to-source (which source?)\n\nCover varying subsets of the language\nHistorical reasons: developed by different people"
  },
  {
    "objectID": "index.html#meaningful-criteria",
    "href": "index.html#meaningful-criteria",
    "title": "Gradients for everyone",
    "section": "Meaningful criteria",
    "text": "Meaningful criteria\n\nDoes this AD backend execute without error?\nDoes it return the right derivative?\nDoes it run fast enough for me?"
  },
  {
    "objectID": "index.html#a-simple-decision-tree",
    "href": "index.html#a-simple-decision-tree",
    "title": "Gradients for everyone",
    "section": "A simple decision tree",
    "text": "A simple decision tree\n\nFollow recommendations of high-level library (e.g. Flux)\nChoose mode based on input and output dimensions\nTry the most battle-tested backends:\n\n\nin forward mode: ForwardDiff or Enzyme\nin reverse mode: Zygote or Enzyme\n\n\nIf nothing works, fall back on finite differences"
  },
  {
    "objectID": "index.html#typical-failures",
    "href": "index.html#typical-failures",
    "title": "Gradients for everyone",
    "section": "Typical failures",
    "text": "Typical failures\n\nFloat64 conversion in ForwardDiff\nMutation in Zygote\nUgly LLVM vomit in Enzyme"
  },
  {
    "objectID": "index.html#forwarddiff-type-genericity",
    "href": "index.html#forwarddiff-type-genericity",
    "title": "Gradients for everyone",
    "section": "ForwardDiff: type-genericity",
    "text": "ForwardDiff: type-genericity\nForwardDiff propagates numbers of type Dual.\n\n\n\nreplace this\nwith that\n\n\n\n\nf(x::Vector{Float64})\nf(x::Vector{&lt;:Real})\n\n\nzeros(n)\nzeros(eltype(x), n)"
  },
  {
    "objectID": "index.html#zygote-custom-rules",
    "href": "index.html#zygote-custom-rules",
    "title": "Gradients for everyone",
    "section": "Zygote: custom rules",
    "text": "Zygote: custom rules\nIf Zygote fails or is too slow, define a custom rule with ChainRulesCore:\nusing ChainRulesCore\n\nfunction ChainRulesCore.rrule(::typeof{myfunc}, x)\n    y = myfunc(x)\n    function myfunc_pullback(dy)\n        df = NoTangent()\n        dx = J'dy # Vector-Jacobian product\n        return (df, dx)\n    end\n    return y, myfunc_pullback\nend\nThe rule returns the primal value and a function for the VJP."
  },
  {
    "objectID": "index.html#enzyme-type-stability-and-activities",
    "href": "index.html#enzyme-type-stability-and-activities",
    "title": "Gradients for everyone",
    "section": "Enzyme: type-stability and activities",
    "text": "Enzyme: type-stability and activities\nEnzyme should be able to differentiate most code, but if not:\n\nImprove type stability\nCheck that you provide temporary storage (with Duplicated) when necessary\nRead the FAQ"
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Gradients for everyone",
    "section": "Goals",
    "text": "Goals\n\nDifferentiationInterface (DI) offers a common syntax for all AD backends1\nAD users can compare correctness and performance without reading each documentation\nAD developers get access to a wider user base\n\n\n\n\n\n\n\nThe fine print\n\n\nDI may be slower than a direct call to the backend’s API (mostly with Enzyme).\n\n\n\ninspired by AbstractDifferentiation"
  },
  {
    "objectID": "index.html#supported-packages",
    "href": "index.html#supported-packages",
    "title": "Gradients for everyone",
    "section": "Supported packages",
    "text": "Supported packages\n\n\n\n\nChainRulesCore\nDiffractor\nEnzyme\nFastDifferentiation\nFiniteDiff\nFiniteDifferences\nForwardDiff\n\n\n\nPolyesterForwardDiff\nReverseDiff\nSymbolics\nTapir\nTracker\nZygote"
  },
  {
    "objectID": "index.html#getting-started-with-di",
    "href": "index.html#getting-started-with-di",
    "title": "Gradients for everyone",
    "section": "Getting started with DI",
    "text": "Getting started with DI\nStep 1: load the necessary packages\n\nusing DifferentiationInterface\nimport ForwardDiff, Enzyme, Zygote\n\nf(x) = sum(abs2, x)\nx = [1.0, 2.0, 3.0, 4.0]\n\nStep 2: Combine DI’s operators with a backend from ADTypes\n\nvalue_and_gradient(f, AutoForwardDiff(), x)\n\n(30.0, [2.0, 4.0, 6.0, 8.0])\n\n\n\nvalue_and_gradient(f, AutoEnzyme(), x)\n\n(30.0, [2.0, 4.0, 6.0, 8.0])\n\n\n\nvalue_and_gradient(f, AutoZygote(), x)\n\n(30.0, [2.0, 4.0, 6.0, 8.0])\n\n\nStep 3: Increase performance via DI’s preparation mechanism"
  },
  {
    "objectID": "index.html#differentiationinterfacetest",
    "href": "index.html#differentiationinterfacetest",
    "title": "Gradients for everyone",
    "section": "DifferentiationInterfaceTest",
    "text": "DifferentiationInterfaceTest\n\nSystematic tests for a variety of inputs and functions\nScenarios with weird arrays (static, GPU, sparse)\nType-stability checks\nAutomated benchmarks"
  },
  {
    "objectID": "index.html#sparse-ad-ecosystem",
    "href": "index.html#sparse-ad-ecosystem",
    "title": "Gradients for everyone",
    "section": "Sparse AD ecosystem",
    "text": "Sparse AD ecosystem\n\nSparseConnectivityTracer for sparsity pattern detection\nSparseMatrixColorings for matrix coloring\nDI for sparse Jacobians and Hessians (see tutorial)\n\n\nSparse AD with coloring (Gebremedhin, Manne, and Pothen 2005)"
  },
  {
    "objectID": "index.html#whats-next",
    "href": "index.html#whats-next",
    "title": "Gradients for everyone",
    "section": "What’s next?",
    "text": "What’s next?\nDI and its sparse AD ecosystem are brand new projects:\n\nTry them out in your code\nReport bugs or inefficiencies\nHelp us improve these packages!\n\n\n\n\n\n\n\nComing up in DI\n\n\nMultiple argument and non-array support."
  },
  {
    "objectID": "index.html#take-home-message",
    "href": "index.html#take-home-message",
    "title": "Gradients for everyone",
    "section": "Take-home message",
    "text": "Take-home message\nComputing derivatives in Julia is easy, but each AD backend comes with its own limitations.\nLearn to recognize and overcome them, either as a user or as a developer."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Gradients for everyone",
    "section": "References",
    "text": "References\n\n\nBlondel, Mathieu, and Vincent Roulet. 2024. “The Elements of Differentiable Programming.” arXiv. https://doi.org/10.48550/arXiv.2403.14606.\n\n\nGebremedhin, Assefaw Hadish, Fredrik Manne, and Alex Pothen. 2005. “What Color Is Your Jacobian? Graph Coloring for Computing Derivatives.” SIAM Review 47 (4): 629–705. https://doi.org/cmwds4."
  }
]